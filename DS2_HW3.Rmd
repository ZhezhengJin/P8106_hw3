---
title: "Homework 3"
author: "Zhezheng Jin"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
--- 

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
```

```{r, echo = TRUE, message = FALSE, results='hide', warning=FALSE}
library(tidyverse)
library(caret)
library(glmnet)
library(MASS)
library(tidymodels)
library(mlbench)
library(pROC)
library(pdp)
library(vip)
library(AppliedPredictiveModeling)
```

```{r}
# Data Import
auto = read_csv("auto.csv") %>%
  mutate(
    mpg_cat = as.factor(mpg_cat),
    origin = as.factor(origin),
    cylinders = as.factor(cylinders)
    ) 

skimr::skim(auto)
# data partition
set.seed(5)
data_split <- initial_split(auto, prop = 0.7)
train <- training(data_split)
test <- testing(data_split)
```

The "auto" dataset contains `r ncol(auto)` columns and `r nrow(auto)` observations. Then we partition the dataset into two parts: training data (70%) and test data (30%), where the training data and test data contains `r nrow(train)` and `r nrow(test)` observations, respectively.

## (a) Logistic Regression
```{r}
contrasts(auto$mpg_cat)

ctrl <- trainControl(method = "cv", number = 10,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

# Using Penalized logistic regression (elastic net for Logistic)
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-6, 2, length = 50)))
set.seed(5)
model.glmn <- train(x = train[1:7],
                    y = train$mpg_cat,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)
model.glmn$bestTune

# Coefficients
coef(model.glmn$finalModel, model.glmn$bestTune$lambda)

# Using caret for comparison
set.seed(5)
model.glm <- train(x = train [1:7],   # exclude the outcome
                   y = train$mpg_cat, # the same as mpg_cat ~ ., data = train
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl)
```

Based on the model coefficients matrix, there are no redundant predictors in the model as all of them have been assigned non-zero coefficients. non-zero coefficients indicate that after the penalization process, all have been deemed relevant to some extent for predicting the outcome variable. 

We first consider the simple classifier with a cut-off of 0.5 and evaluate its performance on the
test data.

## (b) Performance Evaluation
```{r}
test.pred.prob <- predict(model.glmn, newdata = test,
type = "prob") [,2]
test.pred <- rep("high", length(test.pred.prob))
test.pred[test.pred.prob>0.5] <- "low"

confusionMatrix(data = as.factor(test.pred),
                reference = test$mpg_cat,
                positive = "low")
```

Based on our confusion matrix analysis, our model's accuracy when applied to test data is 90.68% (95% CI: 83.83% to 95.25%). No information rate is 53.39%, which represents the accuracy if we made the same class prediction for all observations without any information. The p-value is close to 0 which means the accuracy is statistically significantly better than our no information rate. Our sensitivity (true positives of all actual positives) and specificity (true negatives of all actual negatives) are 85.45% and 95.24%, respectively, with a positive predictive value (true positives of all predicted positives) and negative predictive value (true negatives of all predicted negatives) of 94% and 88.24%, respectively. Additionally, our model demonstrates a balanced accuracy of 90.35%, calculated as the average of our sensitivity and specificity, which indicates good performance in detecting both true positives and true negatives. The high kappa value of 0.8116 suggests a strong inter-rater agreement, even accounting for the possibility of chance agreement.

## (c) MARS
```{r}
set.seed(5)

model.mars <- train(x = train[1:7],
                    y = train$mpg_cat,
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3,nprune = 2:30),
                    metric = "ROC",
                    trControl = ctrl)
plot(model.mars)

model.mars$bestTune %>%
  knitr::kable()
coef(model.mars$finalModel) %>%
  knitr::kable(col.names = "Coefficient")
vip(model.mars$finalModel)

# Confusion Matrix Comparison
mars.pred.prob <- predict(model.mars, newdata = test, type = "prob")[,2]
mars.pred <- rep("high", length(mars.pred.prob))
mars.pred[mars.pred.prob>0.5] <- "low"

matrix <- confusionMatrix(data = as.factor(mars.pred),
                          reference = test$mpg_cat,
                          positive = "low")
matrix

# ROC comparison
res <- resamples(list(MARS = model.mars, 
                      GLM_net = model.glmn))
summary(res)
bwplot(res, metric = "ROC")
```

Based on the confusion matrix comparison, the penalized logistic regression model slightly outperforms the MARS model in predicting the performance for the dataset in question. The improvements are marginal across several key performance metrics, suggesting that while the logistic regression model is preferable in this instance, the difference is not overwhelmingly large.
However, based on the ROC comparsion, the MARS has a higher mean ROC value, which could indeed overturn the initial conclusion based on accuracy, kappa, and other metrics tied to a specific threshold. This would suggest that while the penalized logistic regression may perform slightly better at the particular threshold chosen for classification (leading to higher accuracy, kappa, etc.), the MARS model is more robust overall, with a better capability to distinguish between classes across various thresholds. 

## (d) LDA
```{r}
lda.fit <- lda(mpg_cat~., data = train)

# Plot the linear discriminants in LDA
plot(lda.fit)

# using caret for LDA
set.seed(5)
model.lda <- train(mpg_cat ~ .,
                   data =  train,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)
```

Since we have two classes, we only have one linear discriminant, which allows us to generate a linear discriminant plot displaying our transformed predictors for each class in a histogram.

## (e) Model Comparison
```{r}
res <- resamples(list(LDA = model.lda, 
                      MARS = model.mars, 
                      GLM = model.glm,
                      GLM_net = model.glmn))
summary(res)
bwplot(res, metric = "ROC")
```

Since MARS model has the highest mean ROC, based on the resampling results from how our models perform on the training data, I would use MARS model to predict the response variable `mpg_cat`.

```{r}
# roc
roc.mars <- roc(test$mpg_cat, mars.pred.prob)

plot(roc.mars, legacy.axes = TRUE,  print.auc = TRUE)
plot(smooth(roc.mars), col = 4, add = TRUE)

# Compute the misclassification error rates:
round((1 - (matrix$overall["Accuracy"])),4)
```

From the plot above, the AUC for MARS model is 0.965.The misclassification error rate is about 10.17%.



